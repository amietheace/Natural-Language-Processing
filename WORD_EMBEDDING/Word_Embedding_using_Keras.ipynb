{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word_Embedding_using_Keras.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HEVu37J0gRQ-","colab_type":"text"},"source":["## A Detailed Guide to understand the Word Embeddings and Embedding Layer in Keras.\n","\n","In this notebook I have explained the keras embedding layer. To do so I have created a sample corpus of just 3 documents and that should be sufficient to explain the working of the keras embedding layer.\n","\n","Embeddings are useful in a variety of machine learning applications. Because of the fact I have attached many data sources to the kernel where I fell that embeddings and Keras embedding layer may prove to be useful.\n","\n","Before diving in let us skim through some of the applilcations of the embeddings :\n","\n","1.   The first application that strikes me is in the Collaborative        Filtering based Recommender Systems where we have to create the user      embeddings and the movie embeddings by decomposing the utility            matrix which contains the user-item ratings.\n","\n","2.   The second use is in the Natural Language Processing and its related      applications whre we have to create the word embeddings for all the      words present in the documents of our corpus.\n","\n","\n","\n","Thus the embedding layer in Keras can be used when we want to create the embeddings to embed higher dimensional data into lower dimensional vector space.\n","\n","\n","## 1. Importing Modules"]},{"cell_type":"code","metadata":{"id":"_0RV6EKMgGF3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"8f534a45-2e8d-4b5a-c2d7-52088571ff43","executionInfo":{"status":"ok","timestamp":1563225932270,"user_tz":-330,"elapsed":2690,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["# Ignore  the warnings\n","import warnings\n","warnings.filterwarnings('always')\n","warnings.filterwarnings('ignore')\n","\n","# data visualisation and manipulation\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib import style\n","import seaborn as sns\n","#configure\n","# sets matplotlib to inline and displays graphs below the corressponding cell.\n","%matplotlib inline  \n","style.use('fivethirtyeight')\n","sns.set(style='whitegrid',color_codes=True)\n","\n","#nltk\n","import nltk\n","\n","#stop-words\n","from nltk.corpus import stopwords\n","stop_words=set(nltk.corpus.stopwords.words('english'))\n","\n","# tokenizing\n","from nltk import word_tokenize,sent_tokenize\n","\n","#keras\n","import keras\n","from keras.preprocessing.text import one_hot,Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense , Flatten ,Embedding,Input\n","from keras.models import Model"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"C_ZfRDwTh4VE","colab_type":"text"},"source":["### CREATING SAMPLE CORPUS OF DOCUMENTS ie TEXTS"]},{"cell_type":"code","metadata":{"id":"mvhZQnKohoiS","colab_type":"code","colab":{}},"source":["sample_text_1=\"bitty bought a bit of butter\"\n","sample_text_2=\"but the bit of butter was a bit bitter\"\n","sample_text_3=\"so she bought some better butter to make the bitter butter better\"\n","\n","corp=[sample_text_1,sample_text_2,sample_text_3]\n","no_docs=len(corp)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJJjXq2wiR92","colab_type":"text"},"source":["\n","### INTEGER ENCODING ALL THE DOCUMENTS\n","\n","After this all the unique words will be reprsented by an integer. For this we are using one_hot function from the Keras. Note that the vocab_size is specified large enough so as to ensure unique integer encoding for each and every word.\n","\n","#### Note one important thing that the integer encoding for the word remains same in different docs. eg 'butter' is denoted by 31 in each and every document.\n"]},{"cell_type":"code","metadata":{"id":"Rly9ZWjyiCWK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"0fe6b13e-4031-4934-c039-badcc7cbaa88","executionInfo":{"status":"ok","timestamp":1563226163223,"user_tz":-330,"elapsed":1226,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["vocab_size=50 \n","encod_corp=[]\n","for i,doc in enumerate(corp):\n","    encod_corp.append(one_hot(doc,50))\n","    print(\"The encoding for document\",i+1,\" is : \",one_hot(doc,50))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["The encoding for document 1  is :  [45, 47, 30, 4, 42, 36]\n","The encoding for document 2  is :  [30, 36, 4, 42, 36, 15, 30, 4, 35]\n","The encoding for document 3  is :  [19, 24, 47, 15, 10, 36, 37, 11, 36, 35, 36, 10]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JRVI6pqii_Mv","colab_type":"text"},"source":["### PADDING THE DOCS (to make very doc of same length)\n","\n","The Keras Embedding layer requires all individual documents to be of same length. Hence we wil pad the shorter documents with 0 for now. Therefore now in Keras Embedding layer the 'input_length' will be equal to the length (ie no of words) of the document with maximum length or maximum number of words. \n","\n","#### To pad the shorter documents I am using pad_sequences functon from the Keras library.\n"]},{"cell_type":"code","metadata":{"id":"U0SUeywiikKw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"e4f176b7-88ed-43d5-f424-27a3860f92eb","executionInfo":{"status":"ok","timestamp":1563226468658,"user_tz":-330,"elapsed":1190,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["# length of maximum document. will be nedded whenever create embeddings for the words\n","maxlen=-1\n","for doc in corp:\n","    tokens=nltk.word_tokenize(doc)\n","    if(maxlen<len(tokens)):\n","        maxlen=len(tokens)\n","print(\"The maximum number of words in any document is : \",maxlen)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["The maximum number of words in any document is :  12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8CRJlYEKjuvw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"6f61773c-074b-4dbc-a874-59e70febc79a","executionInfo":{"status":"ok","timestamp":1563226505888,"user_tz":-330,"elapsed":1308,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["# now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.\n","pad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)\n","print(\"No of padded documents: \",len(pad_corp))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["No of padded documents:  3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dgirJeJDj3zk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"fe8adc64-d7a4-4cce-b5e2-935ed3f26628","executionInfo":{"status":"ok","timestamp":1563226552312,"user_tz":-330,"elapsed":773,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["for i,doc in enumerate(pad_corp):\n","     print(\"The padded encoding for document\",i+1,\" is : \",doc)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["The padded encoding for document 1  is :  [45 47 30  4 42 36  0  0  0  0  0  0]\n","The padded encoding for document 2  is :  [30 36  4 42 36 15 30  4 35  0  0  0]\n","The padded encoding for document 3  is :  [19 24 47 15 10 36 37 11 36 35 36 10]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hhlzmx3BkO34","colab_type":"text"},"source":["\n","### ACTUAL CREATION OF THE EMBEDDINGS using KERAS EMBEDDING LAYER\n","\n","Now all the documents are of same length (after padding). And so now we are ready to create and use the embeddings.\n","\n","#### I will embed the words into vectors of 8 dimensions.\n"]},{"cell_type":"code","metadata":{"id":"VA1svE4dkDRN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":128},"outputId":"51c23d14-6567-4d21-8da8-ddb121468436","executionInfo":{"status":"ok","timestamp":1563226714211,"user_tz":-330,"elapsed":1468,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["# specifying the input shape\n","input=Input(shape=(no_docs,maxlen),dtype='float64')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0715 21:38:34.949850 140632073156480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0715 21:38:34.978384 140632073156480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2vhfcRPOkqof","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"3c371c5a-cea5-4686-e10e-f5a8e58d2f0e","executionInfo":{"status":"ok","timestamp":1563226840288,"user_tz":-330,"elapsed":1411,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["'''\n","shape of input. \n","each document has 12 element or words which is the value of our maxlen variable.\n","\n","'''\n","word_input=Input(shape=(maxlen,),dtype='float64')  \n","\n","# creating the embedding\n","word_embedding=Embedding(input_dim=vocab_size,output_dim=8,input_length=maxlen)(word_input)\n","\n","word_vec=Flatten()(word_embedding) # flatten\n","embed_model =Model([word_input],word_vec) # combining all into a Keras model"],"execution_count":10,"outputs":[{"output_type":"stream","text":["W0715 21:40:41.086334 140632073156480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"is_gPTbHlheU","colab_type":"text"},"source":["\n","\n","PARAMETERS OF THE EMBEDDING LAYER ---\n","\n","'input_dim' = the vocab size that we will choose. In other words it is the number of unique words in the vocab.\n","\n","'output_dim' = the number of dimensions we wish to embed into. Each word will be represented by a vector of this much dimensions.\n","\n","'input_length' = lenght of the maximum document. which is stored in maxlen variable in our case.\n"]},{"cell_type":"code","metadata":{"id":"j2SiWNvHlJbI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":165},"outputId":"acdd7b89-3d89-4b11-b85b-bad11439ca42","executionInfo":{"status":"ok","timestamp":1563226970347,"user_tz":-330,"elapsed":1201,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["embed_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),loss='binary_crossentropy',metrics=['acc']) \n","# compiling the model. parameters can be tuned as always."],"execution_count":11,"outputs":[{"output_type":"stream","text":["W0715 21:42:51.356948 140632073156480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0715 21:42:51.365508 140632073156480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","W0715 21:42:51.371954 140632073156480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qNd1XQhPlpOc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"779a2232-e135-433b-b62b-fd28cf7ee571","executionInfo":{"status":"ok","timestamp":1563227014171,"user_tz":-330,"elapsed":1236,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["print(type(word_embedding))\n","print(word_embedding)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["<class 'tensorflow.python.framework.ops.Tensor'>\n","Tensor(\"embedding_1/embedding_lookup/Identity:0\", shape=(?, 12, 8), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZMPECztMlz6g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"2a35605b-35f9-45ff-ce4b-ad2dffae19b0","executionInfo":{"status":"ok","timestamp":1563227034425,"user_tz":-330,"elapsed":1174,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["print(embed_model.summary()) # summary of the model"],"execution_count":13,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 12)                0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 12, 8)             400       \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 96)                0         \n","=================================================================\n","Total params: 400\n","Trainable params: 400\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B42nrCljl44O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"98b21497-cfa2-4038-a8ae-5ebe37290f59","executionInfo":{"status":"ok","timestamp":1563227066824,"user_tz":-330,"elapsed":1878,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["embeddings=embed_model.predict(pad_corp) # finally getting the embeddings."],"execution_count":14,"outputs":[{"output_type":"stream","text":["W0715 21:44:27.131181 140632073156480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"f2T6ilUcmAnS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":908},"outputId":"9cc6c954-c046-49ec-864b-bb1b8b2e6771","executionInfo":{"status":"ok","timestamp":1563227091707,"user_tz":-330,"elapsed":1350,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["print(\"Shape of embeddings : \",embeddings.shape)\n","print(embeddings)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Shape of embeddings :  (3, 96)\n","[[-0.02699658 -0.03079704  0.03379023 -0.04982239  0.00083368  0.00569384\n","   0.04717347  0.04376452 -0.04690733  0.03076388 -0.00690231 -0.03882656\n","  -0.02259206 -0.03037192  0.00665653 -0.01084276  0.01365968  0.03528862\n","  -0.01280572  0.04850254 -0.04555798  0.01974492  0.02806634 -0.00859888\n","  -0.04621972  0.02860166 -0.00810762 -0.00442749 -0.00109739 -0.00604201\n","  -0.01278692 -0.04901706 -0.04353719  0.02420988 -0.03367801  0.04763379\n","   0.04046999 -0.04230182  0.03247703  0.02494914 -0.04569366  0.03029075\n","  -0.04342816 -0.04255356 -0.00922401  0.0213286  -0.03393956  0.03720582\n","  -0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","   0.0436017  -0.02507055 -0.04463922  0.04612506 -0.01916934 -0.01015208\n","   0.00585507 -0.02827555  0.0436017  -0.02507055 -0.04463922  0.04612506\n","  -0.01916934 -0.01015208  0.00585507 -0.02827555  0.0436017  -0.02507055\n","  -0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","   0.0436017  -0.02507055 -0.04463922  0.04612506 -0.01916934 -0.01015208\n","   0.00585507 -0.02827555  0.0436017  -0.02507055 -0.04463922  0.04612506\n","  -0.01916934 -0.01015208  0.00585507 -0.02827555  0.0436017  -0.02507055]\n"," [ 0.01365968  0.03528862 -0.01280572  0.04850254 -0.04555798  0.01974492\n","   0.02806634 -0.00859888 -0.04569366  0.03029075 -0.04342816 -0.04255356\n","  -0.00922401  0.0213286  -0.03393956  0.03720582 -0.04621972  0.02860166\n","  -0.00810762 -0.00442749 -0.00109739 -0.00604201 -0.01278692 -0.04901706\n","  -0.04353719  0.02420988 -0.03367801  0.04763379  0.04046999 -0.04230182\n","   0.03247703  0.02494914 -0.04569366  0.03029075 -0.04342816 -0.04255356\n","  -0.00922401  0.0213286  -0.03393956  0.03720582 -0.02418097  0.03816787\n","  -0.01273117 -0.04063924 -0.0311898   0.03189826 -0.01173266 -0.03729881\n","   0.01365968  0.03528862 -0.01280572  0.04850254 -0.04555798  0.01974492\n","   0.02806634 -0.00859888 -0.04621972  0.02860166 -0.00810762 -0.00442749\n","  -0.00109739 -0.00604201 -0.01278692 -0.04901706 -0.03440434 -0.02704944\n","  -0.02810482  0.03633771  0.02911303 -0.03231921  0.00475172 -0.03760583\n","  -0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","   0.0436017  -0.02507055 -0.04463922  0.04612506 -0.01916934 -0.01015208\n","   0.00585507 -0.02827555  0.0436017  -0.02507055 -0.04463922  0.04612506\n","  -0.01916934 -0.01015208  0.00585507 -0.02827555  0.0436017  -0.02507055]\n"," [ 0.04221157 -0.01180853 -0.01522684  0.01575315 -0.00916978  0.04675757\n","  -0.04910053 -0.02511458  0.04178094 -0.02865592 -0.01074012  0.03545095\n","   0.01115395 -0.00786771 -0.03263132  0.04571399 -0.04690733  0.03076388\n","  -0.00690231 -0.03882656 -0.02259206 -0.03037192  0.00665653 -0.01084276\n","  -0.02418097  0.03816787 -0.01273117 -0.04063924 -0.0311898   0.03189826\n","  -0.01173266 -0.03729881  0.00464379 -0.00451913 -0.0191525   0.03047285\n","  -0.00601838 -0.04855913 -0.04134557  0.04294645 -0.04569366  0.03029075\n","  -0.04342816 -0.04255356 -0.00922401  0.0213286  -0.03393956  0.03720582\n","  -0.00754899  0.01810792 -0.02313652 -0.02649245  0.02058853 -0.0468924\n","  -0.01054697 -0.04250704 -0.00623148 -0.04224439 -0.00198846  0.03471852\n","  -0.04185814  0.00503948 -0.02073139  0.02834275 -0.04569366  0.03029075\n","  -0.04342816 -0.04255356 -0.00922401  0.0213286  -0.03393956  0.03720582\n","  -0.03440434 -0.02704944 -0.02810482  0.03633771  0.02911303 -0.03231921\n","   0.00475172 -0.03760583 -0.04569366  0.03029075 -0.04342816 -0.04255356\n","  -0.00922401  0.0213286  -0.03393956  0.03720582  0.00464379 -0.00451913\n","  -0.0191525   0.03047285 -0.00601838 -0.04855913 -0.04134557  0.04294645]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H-qUKFx7mG0s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"97c2c67b-b0ef-42f6-e78f-9ba1b2dd9b94","executionInfo":{"status":"ok","timestamp":1563227145573,"user_tz":-330,"elapsed":1053,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["embeddings=embeddings.reshape(-1,maxlen,8)\n","print(\"Shape of embeddings : \",embeddings.shape) \n","print(embeddings)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Shape of embeddings :  (3, 12, 8)\n","[[[-0.02699658 -0.03079704  0.03379023 -0.04982239  0.00083368\n","    0.00569384  0.04717347  0.04376452]\n","  [-0.04690733  0.03076388 -0.00690231 -0.03882656 -0.02259206\n","   -0.03037192  0.00665653 -0.01084276]\n","  [ 0.01365968  0.03528862 -0.01280572  0.04850254 -0.04555798\n","    0.01974492  0.02806634 -0.00859888]\n","  [-0.04621972  0.02860166 -0.00810762 -0.00442749 -0.00109739\n","   -0.00604201 -0.01278692 -0.04901706]\n","  [-0.04353719  0.02420988 -0.03367801  0.04763379  0.04046999\n","   -0.04230182  0.03247703  0.02494914]\n","  [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401\n","    0.0213286  -0.03393956  0.03720582]\n","  [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507\n","   -0.02827555  0.0436017  -0.02507055]\n","  [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507\n","   -0.02827555  0.0436017  -0.02507055]\n","  [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507\n","   -0.02827555  0.0436017  -0.02507055]\n","  [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507\n","   -0.02827555  0.0436017  -0.02507055]\n","  [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507\n","   -0.02827555  0.0436017  -0.02507055]\n","  [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507\n","   -0.02827555  0.0436017  -0.02507055]]\n","\n"," [[ 0.01365968  0.03528862 -0.01280572  0.04850254 -0.04555798\n","    0.01974492  0.02806634 -0.00859888]\n","  [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401\n","    0.0213286  -0.03393956  0.03720582]\n","  [-0.04621972  0.02860166 -0.00810762 -0.00442749 -0.00109739\n","   -0.00604201 -0.01278692 -0.04901706]\n","  [-0.04353719  0.02420988 -0.03367801  0.04763379  0.04046999\n","   -0.04230182  0.03247703  0.02494914]\n","  [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401\n","    0.0213286  -0.03393956  0.03720582]\n","  [-0.02418097  0.03816787 -0.01273117 -0.04063924 -0.0311898\n","    0.03189826 -0.01173266 -0.03729881]\n","  [ 0.01365968  0.03528862 -0.01280572  0.04850254 -0.04555798\n","    0.01974492  0.02806634 -0.00859888]\n","  [-0.04621972  0.02860166 -0.00810762 -0.00442749 -0.00109739\n","   -0.00604201 -0.01278692 -0.04901706]\n","  [-0.03440434 -0.02704944 -0.02810482  0.03633771  0.02911303\n","   -0.03231921  0.00475172 -0.03760583]\n","  [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507\n","   -0.02827555  0.0436017  -0.02507055]\n","  [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507\n","   -0.02827555  0.0436017  -0.02507055]\n","  [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507\n","   -0.02827555  0.0436017  -0.02507055]]\n","\n"," [[ 0.04221157 -0.01180853 -0.01522684  0.01575315 -0.00916978\n","    0.04675757 -0.04910053 -0.02511458]\n","  [ 0.04178094 -0.02865592 -0.01074012  0.03545095  0.01115395\n","   -0.00786771 -0.03263132  0.04571399]\n","  [-0.04690733  0.03076388 -0.00690231 -0.03882656 -0.02259206\n","   -0.03037192  0.00665653 -0.01084276]\n","  [-0.02418097  0.03816787 -0.01273117 -0.04063924 -0.0311898\n","    0.03189826 -0.01173266 -0.03729881]\n","  [ 0.00464379 -0.00451913 -0.0191525   0.03047285 -0.00601838\n","   -0.04855913 -0.04134557  0.04294645]\n","  [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401\n","    0.0213286  -0.03393956  0.03720582]\n","  [-0.00754899  0.01810792 -0.02313652 -0.02649245  0.02058853\n","   -0.0468924  -0.01054697 -0.04250704]\n","  [-0.00623148 -0.04224439 -0.00198846  0.03471852 -0.04185814\n","    0.00503948 -0.02073139  0.02834275]\n","  [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401\n","    0.0213286  -0.03393956  0.03720582]\n","  [-0.03440434 -0.02704944 -0.02810482  0.03633771  0.02911303\n","   -0.03231921  0.00475172 -0.03760583]\n","  [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401\n","    0.0213286  -0.03393956  0.03720582]\n","  [ 0.00464379 -0.00451913 -0.0191525   0.03047285 -0.00601838\n","   -0.04855913 -0.04134557  0.04294645]]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9nxNPtV2mdnN","colab_type":"text"},"source":["\n","\n","The resulting shape is (3,12,8).\n","\n","3---> no of documents\n","\n","12---> each document is made of 12 words which was our maximum length of any document.\n","\n","& 8---> each word is 8 dimensional.\n","\n","\n","#### GETTING ENCODING FOR A PARTICULAR WORD IN A SPECIFIC DOCUMENT"]},{"cell_type":"code","metadata":{"id":"cPAgYmbfmUCi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d8c3b512-73ec-4ac5-fac3-98a2a03b75fa","executionInfo":{"status":"ok","timestamp":1563227228117,"user_tz":-330,"elapsed":1272,"user":{"displayName":"amit kumar","photoUrl":"","userId":"13522738909602833953"}}},"source":["for i,doc in enumerate(embeddings):\n","    for j,word in enumerate(doc):\n","        print(\"The encoding for \",j+1,\"th word\",\"in\",i+1,\"th document is : \\n\\n\",word)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["The encoding for  1 th word in 1 th document is : \n","\n"," [-0.02699658 -0.03079704  0.03379023 -0.04982239  0.00083368  0.00569384\n","  0.04717347  0.04376452]\n","The encoding for  2 th word in 1 th document is : \n","\n"," [-0.04690733  0.03076388 -0.00690231 -0.03882656 -0.02259206 -0.03037192\n","  0.00665653 -0.01084276]\n","The encoding for  3 th word in 1 th document is : \n","\n"," [ 0.01365968  0.03528862 -0.01280572  0.04850254 -0.04555798  0.01974492\n","  0.02806634 -0.00859888]\n","The encoding for  4 th word in 1 th document is : \n","\n"," [-0.04621972  0.02860166 -0.00810762 -0.00442749 -0.00109739 -0.00604201\n"," -0.01278692 -0.04901706]\n","The encoding for  5 th word in 1 th document is : \n","\n"," [-0.04353719  0.02420988 -0.03367801  0.04763379  0.04046999 -0.04230182\n","  0.03247703  0.02494914]\n","The encoding for  6 th word in 1 th document is : \n","\n"," [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401  0.0213286\n"," -0.03393956  0.03720582]\n","The encoding for  7 th word in 1 th document is : \n","\n"," [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","  0.0436017  -0.02507055]\n","The encoding for  8 th word in 1 th document is : \n","\n"," [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","  0.0436017  -0.02507055]\n","The encoding for  9 th word in 1 th document is : \n","\n"," [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","  0.0436017  -0.02507055]\n","The encoding for  10 th word in 1 th document is : \n","\n"," [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","  0.0436017  -0.02507055]\n","The encoding for  11 th word in 1 th document is : \n","\n"," [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","  0.0436017  -0.02507055]\n","The encoding for  12 th word in 1 th document is : \n","\n"," [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","  0.0436017  -0.02507055]\n","The encoding for  1 th word in 2 th document is : \n","\n"," [ 0.01365968  0.03528862 -0.01280572  0.04850254 -0.04555798  0.01974492\n","  0.02806634 -0.00859888]\n","The encoding for  2 th word in 2 th document is : \n","\n"," [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401  0.0213286\n"," -0.03393956  0.03720582]\n","The encoding for  3 th word in 2 th document is : \n","\n"," [-0.04621972  0.02860166 -0.00810762 -0.00442749 -0.00109739 -0.00604201\n"," -0.01278692 -0.04901706]\n","The encoding for  4 th word in 2 th document is : \n","\n"," [-0.04353719  0.02420988 -0.03367801  0.04763379  0.04046999 -0.04230182\n","  0.03247703  0.02494914]\n","The encoding for  5 th word in 2 th document is : \n","\n"," [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401  0.0213286\n"," -0.03393956  0.03720582]\n","The encoding for  6 th word in 2 th document is : \n","\n"," [-0.02418097  0.03816787 -0.01273117 -0.04063924 -0.0311898   0.03189826\n"," -0.01173266 -0.03729881]\n","The encoding for  7 th word in 2 th document is : \n","\n"," [ 0.01365968  0.03528862 -0.01280572  0.04850254 -0.04555798  0.01974492\n","  0.02806634 -0.00859888]\n","The encoding for  8 th word in 2 th document is : \n","\n"," [-0.04621972  0.02860166 -0.00810762 -0.00442749 -0.00109739 -0.00604201\n"," -0.01278692 -0.04901706]\n","The encoding for  9 th word in 2 th document is : \n","\n"," [-0.03440434 -0.02704944 -0.02810482  0.03633771  0.02911303 -0.03231921\n","  0.00475172 -0.03760583]\n","The encoding for  10 th word in 2 th document is : \n","\n"," [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","  0.0436017  -0.02507055]\n","The encoding for  11 th word in 2 th document is : \n","\n"," [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","  0.0436017  -0.02507055]\n","The encoding for  12 th word in 2 th document is : \n","\n"," [-0.04463922  0.04612506 -0.01916934 -0.01015208  0.00585507 -0.02827555\n","  0.0436017  -0.02507055]\n","The encoding for  1 th word in 3 th document is : \n","\n"," [ 0.04221157 -0.01180853 -0.01522684  0.01575315 -0.00916978  0.04675757\n"," -0.04910053 -0.02511458]\n","The encoding for  2 th word in 3 th document is : \n","\n"," [ 0.04178094 -0.02865592 -0.01074012  0.03545095  0.01115395 -0.00786771\n"," -0.03263132  0.04571399]\n","The encoding for  3 th word in 3 th document is : \n","\n"," [-0.04690733  0.03076388 -0.00690231 -0.03882656 -0.02259206 -0.03037192\n","  0.00665653 -0.01084276]\n","The encoding for  4 th word in 3 th document is : \n","\n"," [-0.02418097  0.03816787 -0.01273117 -0.04063924 -0.0311898   0.03189826\n"," -0.01173266 -0.03729881]\n","The encoding for  5 th word in 3 th document is : \n","\n"," [ 0.00464379 -0.00451913 -0.0191525   0.03047285 -0.00601838 -0.04855913\n"," -0.04134557  0.04294645]\n","The encoding for  6 th word in 3 th document is : \n","\n"," [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401  0.0213286\n"," -0.03393956  0.03720582]\n","The encoding for  7 th word in 3 th document is : \n","\n"," [-0.00754899  0.01810792 -0.02313652 -0.02649245  0.02058853 -0.0468924\n"," -0.01054697 -0.04250704]\n","The encoding for  8 th word in 3 th document is : \n","\n"," [-0.00623148 -0.04224439 -0.00198846  0.03471852 -0.04185814  0.00503948\n"," -0.02073139  0.02834275]\n","The encoding for  9 th word in 3 th document is : \n","\n"," [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401  0.0213286\n"," -0.03393956  0.03720582]\n","The encoding for  10 th word in 3 th document is : \n","\n"," [-0.03440434 -0.02704944 -0.02810482  0.03633771  0.02911303 -0.03231921\n","  0.00475172 -0.03760583]\n","The encoding for  11 th word in 3 th document is : \n","\n"," [-0.04569366  0.03029075 -0.04342816 -0.04255356 -0.00922401  0.0213286\n"," -0.03393956  0.03720582]\n","The encoding for  12 th word in 3 th document is : \n","\n"," [ 0.00464379 -0.00451913 -0.0191525   0.03047285 -0.00601838 -0.04855913\n"," -0.04134557  0.04294645]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"noEKTh71m-LT","colab_type":"text"},"source":["Now this makes it easier to visualize that we have 3(size of corp) documents with each consisting of 12(maxlen) words and each word mapped to a 8-dimensional vector."]},{"cell_type":"markdown","metadata":{"id":"Kr6G0WyvnOvp","colab_type":"text"},"source":["\n","\n","Just like above we can now use any other document. We can sent_tokenize the doc into sentences.\n","\n","Each sentence has a list of words which we will integer encode using the 'one_hot' function as below.\n","\n","Now each sentence will be having different number of words. So we will need to pad the sequences to the sentence with maximum words.\n","\n","At this point we are ready to feed the input to Keras Embedding layer as shown above.\n","\n","'input_dim' = the vocab size that we will choose\n","\n","'output_dim' = the number of dimensions we wish to embed into\n","\n","'input_length' = lenght of the maximum document\n"]},{"cell_type":"code","metadata":{"id":"LvZggyTpmoI3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}